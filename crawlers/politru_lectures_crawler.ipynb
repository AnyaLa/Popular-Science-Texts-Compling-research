{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Краулер для Politru_lectures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Импортируем все необходимые методы для последующей работы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import urllib.request\n",
    "import bs4 as bs\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Создаем пустой датафрейм. Он пригодится нам в дальнейшем для сортировки данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ready_df = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Скачиваем страницы\n",
    "Функция для скачивания контента по ссылке в виде файла. Аргумент функции - ссылка. Результат работы кода функции - файл в директории \"Pages\". Уникальное имя файла генерируется за счет части url (на основе анализа ссылок на статьи в Indicator.ru) за счет замены / на _ и отсекания общей части для всех url."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def download(url):\n",
    "    source = urllib.request.urlopen(url).read().decode('utf-8')\n",
    "    file_name = url.split('http://polit.ru/article/')[1].replace('/','_')\n",
    "    fl = open('pages_politru_lectures/' + file_name, 'w')\n",
    "    fl.write(str(source))\n",
    "    fl.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Функция находит в словаре ключ по значению"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def find_key(input_dict, value):\n",
    "    return next((k for k, v in input_dict.items() if [value] in v), None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Функция принимает на вход путь к файлу (из ранее сгенерированной директории \"Pages\"). При помощи библиотеки Beautiful Soup на основании предварительного анализа структуры статей сайта находим элементы статьи: \n",
    "- путь до текстового файла на Яндекс-диске\n",
    "- источник\n",
    "- url\n",
    "- дату (дату берем из ссылки на статью при помощи регулярных выражений)\n",
    "- заголовок\n",
    "- подзаголовок\n",
    "- автор \n",
    "- рубрика\n",
    "- тэги\n",
    "- жанр\n",
    "\n",
    "Полученные данные добавляем в заранее заготовленный пустой датафрейм; каждая колонка соответсвует вышеперечисленным элементам"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clear(file_path, dictionary):\n",
    "    \n",
    "    global ready_df\n",
    "    \n",
    "    with open(file_path) as fl:\n",
    "        source = fl.read()\n",
    "    \n",
    "    soup_article = bs.BeautifulSoup(source, 'lxml')\n",
    "    \n",
    "    clear_p = ''\n",
    "    \n",
    "    for paragraph in soup_article.find_all('p'):\n",
    "        clear_p = clear_p + paragraph.text\n",
    "    \n",
    "    head_title = soup_article.find('h1', class_='title').text\n",
    "\n",
    "    rubrics = ''\n",
    "    _rubrics = soup_article.find_all('div', class_= 'tags-subject')\n",
    "    for rub_href in _rubrics:\n",
    "        rub_tag = rub_href.select('a[href]')\n",
    "        for tag in rub_tag:\n",
    "            rubrics = rubrics + '_' + tag.get_text().lower()     \n",
    "    \n",
    "\n",
    "    raw_date = soup_article.find('div', class_='date')\n",
    "    pre_date = re.search(r'.*([0-9]{2}\\s[а-я]+\\s[0-9]{4}).*', str(raw_date))\n",
    "    date = pre_date.group(1)\n",
    "    \n",
    "    source = 'polit.ru/lectures'\n",
    "    \n",
    "    f_name = file_path.replace('pages_politru_lectures/','')\n",
    "    name = f_name.replace('./', '')\n",
    "    url = 'https://polit.ru/article/' + name.replace('_','/')\n",
    "    path = 'politru_lectures/politru_lectures_clear_text/' + name + '.txt'\n",
    "    genre = 'Лекции'\n",
    "    \n",
    "    value = '/article/' + name.replace('_', '/')\n",
    "    author = find_key(dictionary, value)\n",
    "    \n",
    "    subtitle = ''\n",
    "    big_rubrics = ''\n",
    "\n",
    "    df = pd.DataFrame([[path, source, url, date, head_title, subtitle, author, big_rubrics, rubrics, genre]],\n",
    "                      columns=['path', 'source', 'url', 'date','title','subtitle','author','rubrics','tags', 'genre'])\n",
    "    \n",
    "    save_clear_text(clear_p, file_path.replace('pages_politru_lectures/',''))\n",
    "    \n",
    "    ready_df = ready_df.append(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Записываем тексты статей в отдельные файлы\n",
    "Функция принимает в качестве аргумента текст и имя файла для записи (папка clear_text создана заранее). Данная функция понадобится нам в следующей функции."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def save_clear_text(text, file_name):\n",
    "    f = open('politru_lectures_clear_text/'+file_name + '.txt', 'w')\n",
    "    f.write(text)\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Код\n",
    "К адресу сайта При помощи Beuatiful Soup выделяем адреса ссылок по лекторам. Циклом при помощи написанной выше функции скачивания проходим по всем ссылкам лекций и скачиваем их в папку \"Pages_politru_lectures\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lectures_by_authors = urllib.request.urlopen('http://polit.ru/lector/').read().decode('utf-8')\n",
    "soup_links = bs.BeautifulSoup(lectures_by_authors, 'lxml') \n",
    "\n",
    "links = []\n",
    "\n",
    "for author_link in soup_links.find_all('h3', class_ = 'title'):\n",
    "    get_href = str(author_link.select('a[href^=\"/lector/\"]')[0])\n",
    "     \n",
    "    if re.search(r'/lector/\\w+/', get_href) == None:\n",
    "        continue\n",
    "    else:\n",
    "        clear_href = re.findall(r'/lector/\\w+/', get_href)[0]\n",
    "        links.append('http://polit.ru' + clear_href)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Создаем словарь, в котором ключ - имя автора(в формате строки), значения - адреса на лекции этого автора(список строк)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "article_links = []\n",
    "authors = {}\n",
    "\n",
    "for author_dir in links:\n",
    "    author_page = urllib.request.urlopen(author_dir).read().decode('utf-8')\n",
    "    soup_author_page = bs.BeautifulSoup(author_page, 'lxml')\n",
    "    \n",
    "    author = soup_author_page.find('h1', class_='title').text\n",
    "    \n",
    "    \n",
    "    for article_field in soup_author_page.find_all('div', class_ = 'article-tag-result stop'):\n",
    "        clear_href = article_field.select('a[href^=\"/article/\"]')\n",
    "        \n",
    "        for raw_link in clear_href:\n",
    "            article_links.append(re.findall(r'/article/[0-9]{4}/[0-9]{2}/[0-9]{2}/\\w+/', \n",
    "                                            str(raw_link)))\n",
    "            authors.setdefault(author, []).append(re.findall(r'/article/[0-9]{4}/[0-9]{2}/[0-9]{2}/\\w+/', \n",
    "                                            str(raw_link)))\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Скачиваем страницы написанной ранее функцией downlaod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for art_url in article_links:\n",
    "    download('http://polit.ru' + art_url[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сохраняем очищенные тексты при помощи написанной выше функции clear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "start_path = './pages_politru_lectures/'\n",
    "\n",
    "for path, dirs, files in os.walk(start_path):\n",
    "\n",
    "    for fname in files:\n",
    "        if not fname.startswith('.'):\n",
    "            clear(start_path+fname, authors)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сохраняем метаданные в формате csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ready_df.to_csv('politru_lectures.сsv', sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
