{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "from collections import Counter\n",
    "import pymorphy2\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "marked_path = '/home/nst/mount/data/share/yd/popular_science_texts_store/ner_markup/final_markup/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def slurp(path):\n",
    "    with open(path, 'r') as file_object:\n",
    "        return file_object.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_ne(text):\n",
    "    pattern = re.compile(r'&(.*?)!&')\n",
    "    nes = re.findall(pattern, text)\n",
    "    lemmas = []\n",
    "    for ne in nes:\n",
    "        divided = ne.split()\n",
    "        lemma = [morph.parse(word)[0].normal_form for word in divided]\n",
    "        lemma = ' '.join(lemma)\n",
    "        lemmas.append(lemma)\n",
    "    return lemmas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text = 'Есть очень неудачные способы разведения рыб, однако если посмотреть на &Энштейна!& лучшие практики, то они намного меньше вредят природе, чем любое производство на суше», — отметил декан Школы экологии и управления Брена Калифорнийского университета &Стив Гейнс!& в Санта Барбаре'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def range_ne(texts_path, top_number):\n",
    "    nes = []\n",
    "    texts = []\n",
    "    for root, dirs, files in os.walk(texts_path):\n",
    "        for file_name in files:\n",
    "            input_path = texts_path + file_name\n",
    "            marked_text = slurp(input_path)\n",
    "            texts.append(marked_text)\n",
    "    for text in tqdm(texts):\n",
    "        found_ne = extract_ne(text)\n",
    "        #print(found_ne)\n",
    "        nes.extend(found_ne)\n",
    "    ne_dict = Counter(nes)\n",
    "    ranged_ne = list(sorted(ne_dict, key = lambda x : x[1], reverse=True))\n",
    "    print(top_number, \"most cited scholars are:\\n\", ranged_ne[:top_number]) \n",
    "    return ranged_ne"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get NE context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def preprocess(texts):\n",
    "    pattern = re.compile(r'[A-Za-zА-ЯЁа-яё\\-\\.]+')\n",
    "    clean_texts = []\n",
    "    for text in tqdm(texts):\n",
    "        split_text = re.findall(pattern, text)\n",
    "        joined_text = ' '.join(split_text)\n",
    "        clean_texts.append(joined_text)\n",
    "    preproc_texts = pd.DataFrame(clean_texts, columns=['preprocessed_texts'])\n",
    "    return preproc_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 6398.63it/s]\n"
     ]
    }
   ],
   "source": [
    "texts = [text, text_1]\n",
    "preproc = preprocess(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "preproc['marked_texts']=texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>preprocessed_texts</th>\n",
       "      <th>marked_texts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Есть очень неудачные способы разведения рыб од...</td>\n",
       "      <td>Есть очень неудачные способы разведения рыб, о...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Я позволю Девантье продолжить Он начинает со с...</td>\n",
       "      <td>Я позволю Девантье продолжить. Он начинает со ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  preprocessed_texts  \\\n",
       "0  Есть очень неудачные способы разведения рыб од...   \n",
       "1  Я позволю Девантье продолжить Он начинает со с...   \n",
       "\n",
       "                                        marked_texts  \n",
       "0  Есть очень неудачные способы разведения рыб, о...  \n",
       "1  Я позволю Девантье продолжить. Он начинает со ...  "
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preproc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_entities_indices(clean_text, marked_text):\n",
    "    indices = []\n",
    "    entity_index_left = None\n",
    "    entity_index_right = None\n",
    "    pattern = re.compile(r'&(.*?)!&')\n",
    "    nes = re.findall(pattern, marked_text)\n",
    "    print(nes)\n",
    "    for name in nes:\n",
    "        name = name.split()\n",
    "        #метод индекс дает индекс первого символа строки\n",
    "       # try:\n",
    "        if len(name)==1:\n",
    "            name = ''.join(name)\n",
    "            entity_index_left = clean_text.index(name) #индекс начала строки имени\n",
    "            entity_index_right = clean_text.index(name) + len(name) #индекс конца строки\n",
    "                                                                        #имени\n",
    "        else:\n",
    "            entity_index_left = clean_text.index(name[0])\n",
    "            entity_index_right = clean_text.index(name[-1]) + len(name[-1])\n",
    "        index_pair = (entity_index_left, entity_index_right)\n",
    "        indices.append(index_pair)\n",
    "    return indices\n",
    "      #  except ValueError:\n",
    "      #      if len(name)==1:\n",
    "      #          name = ''.join(name)\n",
    "      #          name = name[:-1]\n",
    "      #          entity_index_left = clean_text.index(name)\n",
    "      #          entity_index_right = clean_text.index(name) + len(name)\n",
    "      #      else:\n",
    "      #          name_first = name[0]\n",
    "      #          name_first = name_first[:-1]\n",
    "      #          name_last = name[-1]\n",
    "      #          name_last = name_last[:-1]\n",
    "      #          \n",
    "      #          entity_index_left = clean_text.index(name_first)\n",
    "      #          entity_index_right = clean_text.index(name_last) + len(name_last)\n",
    "      #      index_pair = (entity_index_left, entity_index_right)\n",
    "      #      indices.append(index_pair)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "ind = extract_entities_indices(preproc.preprocessed_texts[1], preproc.marked_texts[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text_1 = 'Я позволю Девантье продолжить. Он начинает со ссылки на исследования &Флойда Тула!& (&Floyd Toole!&), своего бывшего коллеги, и &Шона Олива!& (&Sean Olive!&), уже нынешнего коллеги в Harman, которые когда-то работали в Национальном исследовательском совете Канады:'\\\n",
    "'В 1986 году &Флойд Тул!& и &Шон Олив!& опубликовали исследование о слышимости резонансных колебаний. Они выяснили, что слушатели наиболее чувствительны к низкодобротным (с высокой пропускной способностью) колебаниям. При правильных условиях можно было услышать среднечастотные пики всего в 0,3 децибел (дБ).'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_context(clean_text, entities_indices, window):\n",
    "    #word_index = word_list.index('crisis')\n",
    "    left = []\n",
    "    names = []\n",
    "    right = []\n",
    "    for index_pair in entities_indices:\n",
    "        left_index = index_pair[0]\n",
    "        right_index = index_pair[1] + 1\n",
    "        name = clean_text[left_index:right_index]\n",
    "        names.append(name)\n",
    "        left_context = clean_text[:left_index].split()\n",
    "        if len(left_context) < window:\n",
    "            left_context = clean_text[:left_index]\n",
    "        else:\n",
    "            left_context = ' '.join(left_context[-window:])\n",
    "        left.append(left_context)    \n",
    "        right_context = clean_text[right_index:].split()\n",
    "        if len(right_context) < window:\n",
    "            right_context = clean_text[right_index:]\n",
    "        else:\n",
    "            right_context = ' '.join(right_context[:window+1])\n",
    "        right.append(right_context)\n",
    "    return left, names, right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "left, names, right = get_context(preproc.preprocessed_texts[1], ind, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ссылки на исследования',\n",
       " 'исследования Флойда Тула',\n",
       " 'бывшего коллеги и',\n",
       " 'и Шона Олива',\n",
       " 'ссылки на исследования',\n",
       " 'бывшего коллеги и']"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "left"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. entry values: clean text, marked_text, window\n",
    "2. Make a list of indices for every text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_contexts(clean_texts, marked_texts, window):\n",
    "    indices_list = [extract_entities_indices(clean_text, marked_text)\n",
    "              for clean_text, marked_text in zip(clean_texts, marked_texts)]\n",
    "    lefts_list = []\n",
    "    names_list = []\n",
    "    rights_list = []\n",
    "    for text, indices in zip(clean_texts, indices_list):\n",
    "        left, names, right = get_context(text, indices, window)\n",
    "        lefts_list.extend(left)\n",
    "        names_list.extend(names)\n",
    "        rights_list.extend(right)\n",
    "    dataframe = pd.DataFrame(lefts_list, columns=['left_context'])\n",
    "    dataframe['named_entities'] = names_list\n",
    "    dataframe['right_context'] = rights_list\n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Энштейна', 'Стив Гейнс']\n",
      "['Флойда Тула', 'Floyd Toole', 'Шона Олива', 'Sean Olive', 'Флойд Тул', 'Шон Олив']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>left_context</th>\n",
       "      <th>named_entities</th>\n",
       "      <th>right_context</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>рыб однако если посмотреть на</td>\n",
       "      <td>Энштейна</td>\n",
       "      <td>лучшие практики то они намного меньше</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>и управления Брена Калифорнийского университета</td>\n",
       "      <td>Стив Гейнс</td>\n",
       "      <td>в Санта Барбаре</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>начинает со ссылки на исследования</td>\n",
       "      <td>Флойда Тула</td>\n",
       "      <td>Floyd Toole своего бывшего коллеги и</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ссылки на исследования Флойда Тула</td>\n",
       "      <td>Floyd Toole</td>\n",
       "      <td>своего бывшего коллеги и Шона Олива</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Toole своего бывшего коллеги и</td>\n",
       "      <td>Шона Олива</td>\n",
       "      <td>Sean Olive уже нынешнего коллеги в</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>бывшего коллеги и Шона Олива</td>\n",
       "      <td>Sean Olive</td>\n",
       "      <td>уже нынешнего коллеги в Harman которые</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>начинает со ссылки на исследования</td>\n",
       "      <td>Флойда Тула</td>\n",
       "      <td>Floyd Toole своего бывшего коллеги и</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Toole своего бывшего коллеги и</td>\n",
       "      <td>Шона Олива</td>\n",
       "      <td>Sean Olive уже нынешнего коллеги в</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      left_context named_entities  \\\n",
       "0                    рыб однако если посмотреть на      Энштейна    \n",
       "1  и управления Брена Калифорнийского университета    Стив Гейнс    \n",
       "2               начинает со ссылки на исследования   Флойда Тула    \n",
       "3               ссылки на исследования Флойда Тула   Floyd Toole    \n",
       "4                   Toole своего бывшего коллеги и    Шона Олива    \n",
       "5                     бывшего коллеги и Шона Олива    Sean Olive    \n",
       "6               начинает со ссылки на исследования    Флойда Тула   \n",
       "7                   Toole своего бывшего коллеги и     Шона Олива   \n",
       "\n",
       "                            right_context  \n",
       "0   лучшие практики то они намного меньше  \n",
       "1                         в Санта Барбаре  \n",
       "2    Floyd Toole своего бывшего коллеги и  \n",
       "3     своего бывшего коллеги и Шона Олива  \n",
       "4      Sean Olive уже нынешнего коллеги в  \n",
       "5  уже нынешнего коллеги в Harman которые  \n",
       "6    Floyd Toole своего бывшего коллеги и  \n",
       "7      Sean Olive уже нынешнего коллеги в  "
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_all_contexts(preproc.preprocessed_texts, preproc.marked_texts, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make Dataframe and get all NE contexts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Marked & Preprocessed DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_texts_df(texts_path):\n",
    "    marked_texts = []\n",
    "    for root, dirs, files in os.walk(texts_path):\n",
    "        for file_name in files:\n",
    "            input_path = texts_path + file_name\n",
    "            marked_text = slurp(input_path)\n",
    "            marked_texts.append(marked_text)\n",
    "    marked_df = pd.DataFrame(marked_texts, columns=['marked_texts'])\n",
    "    return marked_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "marked_df = make_texts_df(marked_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 165/165 [00:00<00:00, 956.42it/s]\n"
     ]
    }
   ],
   "source": [
    "preprocessed_texts = preprocess(marked_df.marked_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts_df = marked_df.join(preprocessed_texts) ;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Радиопульсары одно из наблюдательных проявлений нейтронных звезд источники пульсирующего радиоизлучения с периодами от нескольких миллисекунд до секунд. Они были открыты лет назад. Пожалуй широкомасштабное наблюдательное изучение нейтронных звёзд началось именно с открытия радиопульсаров в году в Маллардской обсерватории в Великобритании. Это было сделано во время исследований мерцаний радиоисточников. Первооткрывателями этих объектов можно назвать тогдашнюю аспирантку Джоселин Белл и ее руководителя Энтони Хьюиша . Причем понимание того с источниками какой природы они имеют дело пришло не сразу. Хьюиш если верить пересказам сначала не вполне поверил что первый открытый объект был именно космического природного происхождения и не был артефактом. И даже когда таких источников обнаружилось несколько им было присвоено обозначение LGM Little Green Men маленькие зелёные человечки . То есть был очень большой соблазн из-за строгой периодичности излучения говорить о том что это сигналы разумного происхождения сигналы внеземных цивилизаций. Однако уже через несколько месяцев первооткрыватели поняли что есть по крайней мере в теории природный объект который может объяснить такую стабильную периодичность это вращающиеся нейтронные звёзды с массой порядка солнечной но очень компактные очень плотные релятивистские объекты вращающиеся с очень большой инертностью обеспечивающей стабильность периода. В году за это открытие Э. Хьюишу была присуждена Нобелевская премия по физике. Джоселин Белл в список лауреатов не вошла. На сегодняшний день известно почти две тысячи радиопульсаров. Это не единственное наблюдательное проявление нейтронных звезд но самое массовое. И изучение их природы изучение столь плотного вещества которое их составляет изучение поведения вещества в сверхсильных магнитных полях которые им сопутствуют сверхсильной гравитации которая рядом с этими компактными объектами имеет место связано с изучением именно радиопульсаров. Физика радиопульсаров достаточно сложна и какой-то законченной и общепринятой их модели еще пока нет. Но все же мы достаточно хорошо понимаем как они устроены. Мы понимаем что у радиопульсаров есть очень сильное магнитное поле которое порой мы даже можем измерять напрямую по линиям так называемого циклотронного излучения. Однако есть очень простой способ измерить его не напрямую. Период вращения радиопульсара не остается постоянным. Потихоньку энергия вращения уходит в частицы в излучение кстати механизм преобразования в радиоизлучение до конца ещё не ясен и пульсар постепенно замедляется. Темп замедления во многом определяется силой магнитного поля. Измеряя одно мы получаем оценку и для второго. При этом оценки которые мы получаем для реальных пульсаров очень хорошо совпадают с теоретическими предсказаниями следующими из наших представлений об образовании этих объектов. Мы знаем что пульсары образуются во время вспышек сверхновых звёзд. Нейтронная звезда это сколлапсировавшее ядро звезды причем вылетевшее после взрыва с очень большой скоростью. Потому что по видимому взрыв асимметричен. Такие звезды одни из самых быстрых объектов нашей Галактике. Средняя скорость нейтронных звёзд составляет км с. Зная возраст нейтронной звезды который мы также можем оценить из темпа изменения ее периода и параметры ее движения можно проследить её путь назад до места где она родилась и убедиться что там находится остаток сверхновой. Если конечно он еще не успел рассеяться. Радиопульсары полезны ещё и для другой области астрономии для исследования межзвёздной среды. Дело в том что радиоизлучение распространяется в плотной межзвёздной среде не в пустоте с разной скоростью в зависимости от длины волны. В результате составляющие радиоимпульса излучённого пульсаром соответствующие разным длинам волн доходят до наблюдателя не одновременно а с некоторой задержкой. Измеряя эту задержку мы можем сказать какова плотность той межзвёздной среды через которую этот импульс проходил. Таким же образом мы можем оценить расстояние до пульсара. Поскольку чем больший путь прошел импульс тем больше задержка между его составляющими. Всего в Галактике исходя из теории образования нейтронных звёзд может насчитываться порядка этих объектов. Из них до ста тысяч могут в принципе наблюдаться. Мы же сейчас знаем всего лишь две тысячи. Чтобы увеличить это число уже сейчас строятся большие телескопы. Один из них это телескоп SKA Square Kilometre Array телескоп который будет иметь эффективную площадь км . Ещё одно направление которое сейчас является достаточно перспективным - это поиск длинноволнового гравитационного излучения при помощи пульсаров. Если мы разместим два пульсара в галактике и через него пройдёт гравитационная волна то эти пульсары начнут немного колебаться и их наблюдаемый период который нам известен с очень высокой точностью у некоторых пульсаров с точностью до - сек тоже начнёт немного колебаться просто в силу эффекта Доплера. Если это колебание у нескольких пульсаров окажется согласованным то мы можем сказать что да мы нашли некий след от прошедшей через них гравитационной волны. Была сделана специальная выборка пульсаров за которыми следят астрономы практически каждый день именно для них ищутся вот эти возможные колебания их периодов. Пока их ещё не нашли но возможно что неуловимые по сей день гравитационные волны впервые будут открыты именно при помощи радиопульсаров.'"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts_df.preprocessed_texts[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get all contexts from marked texts with window 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Ричарда Фейнмана', 'Гердом Биннигом', 'Генрихом Рорером', 'Дональдом Эйглером', 'Эрхардом Швейцером', 'Джозефа Стросцио', 'Дональд Эйглер', 'Первое', 'Капоши']\n",
      "['Первое', 'Андрей Анатольевич Зализняк', 'Зализняка', 'Ирины Левонтиной', 'Максима Кронгауза', 'Кронгауз', 'Панова', 'Маркса', 'Эйнштейна', 'Логунове', 'Зализняк', 'Зализняка']\n",
      "['Джоселин Белл', 'Энтони Хьюиша', 'Хьюиш', 'Э. Хьюишу', 'Джоселин Белл']\n",
      "['Скажем', 'Ричарда Ф. Фейнмана', 'Фейнман', 'Менделеева', 'Максвелл', 'Эйнштейн']\n",
      "['Ричарда Форти', 'Форти', 'Гарри Уиттингтон', 'Дерек Бриггс', 'Саймон Конвей Моррис', 'Уолкотта', 'Гарри Уиттингтон', 'Саймона Конвея Морриса', 'Саймону Конвею Моррису', 'Саймону Конвею Моррису', 'Гарри Уиттингтона', 'Саймон Конвей Моррис', 'Дерек Бриггс', 'Дэвид Брутон', 'Крис Хьюз', 'Кембридж', 'Дереком Бриггсом', 'Стивеном Гулдом', 'Дреком Бриггсом', 'Дэвид Суоффорд', 'Суоффорд', 'Хокинг', 'Мэтью Уилз', 'Ричарда Фейнмана', 'Стивена Хокинга', 'Грег Эджкомб']\n",
      "['Бориса Родомана', 'Борис Родоман', 'Глазычев', 'Родоман', 'Найшуль', 'Орешкин', 'Дмитрий Орешкин', 'Даниил Дондурей', 'Алексей Ханютин', 'Андрей Зорин', 'Сергей Хоружий', 'Вячеслав Глазычев', ' и &Александр Сарычев', 'Андрей Зорин', 'Алексей Левинсон', 'Юрий Шмидт', 'Александр Аузан', 'Симон Кордонский', 'Сергей Сельянов', 'Виталий Найшуль', 'Юрий Левада', 'Олег Генисаретский', 'Махмут Гареев', 'Леонид Смирнягин', 'Борис Родоман', 'Алексей Титков', 'Пол Куртц']\n",
      "['ЭмильБерлинер', 'ТомасаЭдисона', 'Чичестера Белла', 'Александра Грэхема Белла', 'Чарльза Тейнтера', 'Берлинер', 'Берлинера']\n",
      "['Михаила Городецкого', 'Тобиаса Киппенберга', 'Григорий Лихачев', 'Лихачев', 'Лихачев']\n",
      "['Хокинг', 'Стивен Хокинг', 'Стивеном Хокингом', 'Пол Нуюжукян', 'Джейми Хендерсоном']\n",
      "['Райан Бигелоу', 'Стив Гейнс']\n",
      "['Рой Глаубер', 'Roy Glauber', 'России']\n",
      "['Михаил Пантелеев', 'Михаил Пантелеев', 'Михаилом Пантелеевым', 'Михаил Пантелеев', 'Михаил Пантелеев', 'Михаил Пантелеев', 'Михаил Пантелеев']\n",
      "['Может', 'Дадли Хершбаха', 'Эрика Маскина', 'Ричарда Робертса', 'Роя Глаубера', 'Ахмеду Шафику', 'Шафик', 'Шафик', 'Шафик', 'Марком Ависом', 'Сарой Форб', 'Шилой Фергюсон', 'Это', 'Кристоф Хельмчен', 'Карина Палцер', 'Томас Мюнте', 'Шилке Андрес', 'Андреас Шпренгер', 'Чарльзу Фостеру', 'Томасу Твейтсу', 'Фостер', 'Фостер', 'Фостер', 'Томаса Твейтса', 'Твейтс', 'Твейтс', 'Фредрику Шебергу', 'Шеберга', 'Тумас Транстремер', 'Ацуки Хигасияма', 'Кохей Адачи']\n",
      "['Виталий Лазаревич Гинзбург', 'Гинзбург', 'Вячеслав Муханов', 'Виталии Лазаревиче', 'Виталия Лазаревича', 'Гинзбург', 'Гинзбурга', 'Виталий Лазаревич', 'Эйнштейн', 'Виталий Лазаревич', 'Гинзбург', 'Гинзбург', 'Не', 'Гинзбурга', 'Виталия Лазаревича', 'Виталий Лазаревич', 'Гинзбургом', 'Гинзбург', 'Виталия Лазарвича', 'Виталий Лазаревич', 'Альберт Эйнштейн', 'Максе Планке', 'Виталий Лазаревич']\n",
      "['Владик Аветисов', 'Фейнмане', 'Соважа', 'Стоддарта', 'Феринги', 'Фейнманом', 'Ричард Фейнман', 'Гросберг', 'Нечаев', 'Шахнович', 'Мюллер', 'Бедронц', 'Можно']\n",
      "['Шарля Монтескьё', 'Вольтера', 'Вольтером', 'Дени Дидро', 'Адамом Смитом']\n",
      "['Эванджелиста Торричелли', 'Торричелли', 'Бенедетто Кастелли', 'Галилея', 'Кастелли', 'Торричелли', 'Галилеем', 'Кастелли', 'Галилею', 'Галилеем', 'Торричелли', 'Галилеем', 'Торричелли', 'Кастелли', 'Галилею', 'Торричелли', 'Галилея', 'Торричелли', 'Торричелли', 'Кастелли', 'Бонавентуры Кавальери', 'Торричелли', 'Торричелли', 'Галилея', 'Торричелли', 'Торричелли', 'Микеланджело Риччи']\n",
      "['Ломоносовым', 'Леонид Ксанфомалити']\n",
      "[]\n",
      "['Эркки Куренниеми', 'Куренниеми', 'Куренниеми', 'Куренниеми', 'Саймон Рейнолдс', 'Куренниеми', 'Эркки Йоханнес Куренниеми', 'Тауно Куренниеми', 'Куренниеми', 'Куренниеми', 'Эрик Тавастстерна', 'Куренниеми', 'Куренниеми', 'Куренниеми', 'Куренниеми', 'Куренниеми', 'Куренниеми', 'Куренниеми', 'Куренниеми', 'Куренниеми', 'Куренниеми', 'Куркиниеми', 'Куренниеми', 'Эркки Куренниеми', 'Эркки Куренниеми', 'Эркки Йоханнес Куренниеми', 'Куренниеми', 'Куренниеми', 'Куренниеми']\n",
      "[]\n",
      "['Майкл Катерино', 'Michael S. Caterino', 'Карин Вольф-Швеннингер', 'Katrin Wolf-Schwenniger', 'Гюнтер Бехли', 'Günter Bechly', 'Майкл Катерино', 'Майкл Катерино']\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "substring not found",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-164-7c4a43102bf9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcontexts_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_all_contexts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtexts_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessed_texts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtexts_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmarked_texts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-95-9de60c391f37>\u001b[0m in \u001b[0;36mget_all_contexts\u001b[0;34m(clean_texts, marked_texts, window)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_all_contexts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclean_texts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmarked_texts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwindow\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     indices_list = [extract_entities_indices(clean_text, marked_text)\n\u001b[0;32m----> 3\u001b[0;31m               for clean_text, marked_text in zip(clean_texts, marked_texts)]\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mlefts_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mnames_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-95-9de60c391f37>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_all_contexts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclean_texts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmarked_texts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwindow\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     indices_list = [extract_entities_indices(clean_text, marked_text)\n\u001b[0;32m----> 3\u001b[0;31m               for clean_text, marked_text in zip(clean_texts, marked_texts)]\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mlefts_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mnames_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-163-c7174fa521b0>\u001b[0m in \u001b[0;36mextract_entities_indices\u001b[0;34m(clean_text, marked_text)\u001b[0m\n\u001b[1;32m     16\u001b[0m                                                                         \u001b[0;31m#имени\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m             \u001b[0mentity_index_left\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclean_text\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m             \u001b[0mentity_index_right\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclean_text\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mindex_pair\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mentity_index_left\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mentity_index_right\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: substring not found"
     ]
    }
   ],
   "source": [
    "contexts_df = get_all_contexts(texts_df.preprocessed_texts, texts_df.marked_texts, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['K.', 'Wolf-Schwenniger']"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'K. Wolf-Schwenniger'.split()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
