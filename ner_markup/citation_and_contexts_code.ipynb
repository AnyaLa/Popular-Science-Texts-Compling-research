{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "from collections import Counter\n",
    "import pymorphy2\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import pandas as pd\n",
    "import math\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "marked_path = '/home/nst/mount/data/share/yd/popular_science_texts_store/ner_markup/final_markup/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def slurp(path):\n",
    "    with open(path, 'r') as file_object:\n",
    "        return file_object.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_ne(text):\n",
    "    pattern = re.compile(r'&(.*?)!&')\n",
    "    nes = re.findall(pattern, text)\n",
    "    lemmas = []\n",
    "    for ne in nes:\n",
    "        divided = ne.split()\n",
    "        lemma = [morph.parse(word)[0].normal_form for word in divided]\n",
    "        lemma = ' '.join(lemma)\n",
    "        lemmas.append(lemma)\n",
    "    return lemmas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def range_ne(texts_path, top_number):\n",
    "    nes = []\n",
    "    texts = []\n",
    "    for root, dirs, files in os.walk(texts_path):\n",
    "        for file_name in files:\n",
    "            input_path = texts_path + file_name\n",
    "            marked_text = slurp(input_path)\n",
    "            texts.append(marked_text)\n",
    "    for text in tqdm(texts):\n",
    "        found_ne = extract_ne(text)\n",
    "        #print(found_ne)\n",
    "        nes.extend(found_ne)\n",
    "    ne_dict = Counter(nes)\n",
    "    ranged_ne = list(sorted(ne_dict, key = lambda x : x[1], reverse=True))\n",
    "    print(top_number, \"most cited scholars are:\\n\", ranged_ne[:top_number]) \n",
    "    return ranged_ne"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get NE context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def preprocess(texts):\n",
    "    #pattern = re.compile(r'[\\w.-]+')\n",
    "    clean_texts = []\n",
    "    for text in tqdm(texts):\n",
    "        split_text = re.findall(r'[\\w.\\'-]+', text, flags=re.UNICODE)\n",
    "        joined_text = ' '.join(split_text)\n",
    "        clean_texts.append(joined_text)\n",
    "    preproc_texts = pd.DataFrame(clean_texts, columns=['preprocessed_texts'])\n",
    "    return preproc_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_entities_indices(clean_text, marked_text):\n",
    "    indices = []\n",
    "    entity_index_left = None\n",
    "    entity_index_right = None\n",
    "    pattern = re.compile(r'&(.*?)!&')\n",
    "    nes = re.findall(pattern, marked_text)\n",
    "   # print(nes)\n",
    "    for name in nes:\n",
    "        name = name.split()\n",
    "        if len(name)==1:\n",
    "            name = ''.join(name)\n",
    "            try:\n",
    "                entity_index_left = clean_text.index(name)\n",
    "            except ValueError:\n",
    "                print('Name:', name)\n",
    "                print('Text_marked:', marked_text)\n",
    "                raise \n",
    "            try:\n",
    "                entity_index_right = clean_text.index(name) + len(name) \n",
    "            except ValueError:\n",
    "                print('Name:', name)\n",
    "                print('Text_marked:', marked_text)\n",
    "                raise                             \n",
    "        else:\n",
    "            try:\n",
    "                entity_index_left = clean_text.index(name[0])\n",
    "            except ValueError:\n",
    "                print('Name:', name[0])\n",
    "                print('Text_marked:', marked_text)\n",
    "                raise \n",
    "            try:\n",
    "                entity_index_right = clean_text.index(name[-1]) + len(name[-1])\n",
    "            except ValueError:\n",
    "                print('Name:', name[0])\n",
    "                print('Text_marked:', marked_text)\n",
    "                raise \n",
    "        index_pair = (entity_index_left, entity_index_right)\n",
    "        indices.append(index_pair)\n",
    "    return indices "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_context(clean_text, entities_indices, window):\n",
    "    left = []\n",
    "    names = []\n",
    "    right = []\n",
    "    for index_pair in entities_indices:\n",
    "        left_index = index_pair[0]\n",
    "        right_index = index_pair[1] + 1\n",
    "        name = clean_text[left_index:right_index]\n",
    "        names.append(name)\n",
    "        left_context = clean_text[:left_index].split()\n",
    "        if len(left_context) < window:\n",
    "            left_context = clean_text[:left_index]\n",
    "        else:\n",
    "            left_context = ' '.join(left_context[-window:])\n",
    "        left.append(left_context)    \n",
    "        right_context = clean_text[right_index:].split()\n",
    "        if len(right_context) < window:\n",
    "            right_context = clean_text[right_index:]\n",
    "        else:\n",
    "            right_context = ' '.join(right_context[:window+1])\n",
    "        right.append(right_context)\n",
    "    return left, names, right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_all_contexts(clean_texts, marked_texts, window):\n",
    "    indices_list = [extract_entities_indices(clean_text, marked_text)\n",
    "              for clean_text, marked_text in zip(clean_texts, marked_texts)]\n",
    "    lefts_list = []\n",
    "    names_list = []\n",
    "    rights_list = []\n",
    "    for text, indices in zip(clean_texts, indices_list):\n",
    "        left, names, right = get_context(text, indices, window)\n",
    "        lefts_list.extend(left)\n",
    "        names_list.extend(names)\n",
    "        rights_list.extend(right)\n",
    "    dataframe = pd.DataFrame(lefts_list, columns=['left_context'])\n",
    "    dataframe['named_entities'] = names_list\n",
    "    dataframe['right_context'] = rights_list\n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make Dataframe and get all NE contexts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Marked & Preprocessed DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_texts_df(texts_path):\n",
    "    marked_texts = []\n",
    "    for root, dirs, files in os.walk(texts_path):\n",
    "        for file_name in files:\n",
    "            input_path = texts_path + file_name\n",
    "            marked_text = slurp(input_path)\n",
    "            marked_texts.append(marked_text)\n",
    "    marked_df = pd.DataFrame(marked_texts, columns=['marked_texts'])\n",
    "    return marked_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "marked_df = make_texts_df(marked_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 164/164 [00:00<00:00, 750.44it/s]\n"
     ]
    }
   ],
   "source": [
    "preprocessed_texts = preprocess(marked_df.marked_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "texts_df = marked_df.join(preprocessed_texts) ;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "texts_df.preprocessed_texts[2] ;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get all contexts from marked texts with window 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "contexts_df = get_all_contexts(texts_df.preprocessed_texts, texts_df.marked_texts, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1903"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(contexts_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "remove_duplicates = contexts_df.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1255"
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(remove_duplicates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>left_context</th>\n",
       "      <th>named_entities</th>\n",
       "      <th>right_context</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Из выступления лауреата Нобелевской премии</td>\n",
       "      <td>Ричарда Фейнмана</td>\n",
       "      <td>в Калифорнийском технологическом институте в 1959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>проблема была в 1981 году</td>\n",
       "      <td>Гердом Биннигом</td>\n",
       "      <td>и Генрихом Рорером сотрудниками швейцарского о...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1981 году Гердом Биннигом и</td>\n",
       "      <td>Генрихом Рорером</td>\n",
       "      <td>сотрудниками швейцарского отделения IBM сконст...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>сотрудниками IBM Almaden Research Center</td>\n",
       "      <td>Дональдом Эйглером</td>\n",
       "      <td>и Эрхардом Швейцером в 1990 году</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Research Center Дональдом Эйглером и</td>\n",
       "      <td>Эрхардом Швейцером</td>\n",
       "      <td>в 1990 году заключался в том</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 left_context       named_entities  \\\n",
       "0  Из выступления лауреата Нобелевской премии    Ричарда Фейнмана    \n",
       "1                   проблема была в 1981 году     Гердом Биннигом    \n",
       "2                 1981 году Гердом Биннигом и    Генрихом Рорером    \n",
       "3    сотрудниками IBM Almaden Research Center  Дональдом Эйглером    \n",
       "4        Research Center Дональдом Эйглером и  Эрхардом Швейцером    \n",
       "\n",
       "                                       right_context  \n",
       "0  в Калифорнийском технологическом институте в 1959  \n",
       "1  и Генрихом Рорером сотрудниками швейцарского о...  \n",
       "2  сотрудниками швейцарского отделения IBM сконст...  \n",
       "3                   и Эрхардом Швейцером в 1990 году  \n",
       "4                       в 1990 году заключался в том  "
      ]
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "remove_duplicates = remove_duplicates.reset_index(drop=True)\n",
    "remove_duplicates.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "remove_duplicates.to_csv('contexts_draft.tsv', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "company_audiomania_blog_251144.txt\n"
     ]
    }
   ],
   "source": [
    "for root, dirs, files in os.walk(marked_path):\n",
    "    for file_name in files:\n",
    "        input_path = marked_path + file_name\n",
    "        marked_text = slurp(input_path)\n",
    "        if 'Субдискретизация' in marked_text:\n",
    "            print(file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Когда &Гарри Уиттингтон!& и его ученики &Дерек Бриггс!& и &Саймон Конвей Моррис!&'"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'Когда &Гарри Уиттингтон!& и его ученики &Дерек Бриггс!& и &Саймон Конвей Моррис!&'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('/home/nst/mount/data/linguistics_hse/popular-science-research/popular-science-repo/Popular-Science-Texts-Compling-research/ner_markup/contexts_draft.tsv', sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Count most frequent collocations in contexts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def preprocess(texts):\n",
    "    preproc_texts = []\n",
    "    pattern = re.compile(r'\\w+')\n",
    "    try:\n",
    "        for text in tqdm(texts):\n",
    "            text = text.lower()\n",
    "            words = ' '.join(re.findall(pattern, text))\n",
    "            preproc_texts.append(words)\n",
    "    except AttributeError:\n",
    "        print('Corrupted:', text)\n",
    "        preproc_texts.append('')\n",
    "    return preproc_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1216/1216 [00:00<00:00, 93385.95it/s]\n",
      "100%|██████████| 1216/1216 [00:00<00:00, 93964.03it/s]\n"
     ]
    }
   ],
   "source": [
    "preprocessed_left = preprocess(df.left_context)\n",
    "preprocessed_right = preprocess(df.right_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1216"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.dropna(axis=0, how='any')\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>left</th>\n",
       "      <th>right</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>из выступления лауреата нобелевской премии</td>\n",
       "      <td>в калифорнийском технологическом институте в 1959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>проблема была в 1981 году</td>\n",
       "      <td>и генрихом рорером сотрудниками швейцарского о...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1981 году гердом биннигом и</td>\n",
       "      <td>сотрудниками швейцарского отделения ibm сконст...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         left  \\\n",
       "0  из выступления лауреата нобелевской премии   \n",
       "1                   проблема была в 1981 году   \n",
       "2                 1981 году гердом биннигом и   \n",
       "\n",
       "                                               right  \n",
       "0  в калифорнийском технологическом институте в 1959  \n",
       "1  и генрихом рорером сотрудниками швейцарского о...  \n",
       "2  сотрудниками швейцарского отделения ibm сконст...  "
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessed_df = pd.DataFrame(preprocessed_left, columns=['left'])\n",
    "preprocessed_df['right'] = preprocessed_right\n",
    "preprocessed_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compile_tokens_list(texts):\n",
    "    tokens = []\n",
    "    for text in texts:\n",
    "        text = text.split()\n",
    "    tokens.extend(text)\n",
    "    tokens = ' '.join(tokens)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tokens_left = compile_tokens_list(preprocessed_df.left)\n",
    "tokens_right = compile_tokens_list(preprocessed_df.right)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def aprior_probability(dictionary):\n",
    "    s = sum(dictionary.values())\n",
    "    for word  in dictionary:\n",
    "        dictionary[word] /= s\n",
    "   # aprior_dictionary = dict(wiki_freq).get(word)/s\n",
    "    return dict(dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def bigram_probability(bigrams):\n",
    "    count_bigrams = len(bigrams)\n",
    "    joined_bigrams = [' '.join(bigram) for bigram in bigrams]\n",
    "    bigrams_dict = Counter(joined_bigrams)\n",
    "    for bigram in bigrams_dict:\n",
    "        bigrams_dict[bigram] /= count_bigrams\n",
    "    return dict(bigrams_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def count_pmi(bigrams, bigrams_prob, words_aprior):\n",
    "    joined_bigrams = []\n",
    "    pmis = []\n",
    "    for bigram in bigrams:\n",
    "        #print(bigram)\n",
    "        joined_bigram = ' '.join(bigram)\n",
    "        pmi = math.log(bigrams_prob.get(joined_bigram)/\n",
    "                      (words_aprior.get(bigram[0])*words_aprior.get(bigram[1])))\n",
    "        joined_bigrams.append(joined_bigram)\n",
    "        pmis.append(pmi)\n",
    "    counted_pmi = list(zip(joined_bigrams, pmis))\n",
    "    return counted_pmi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def range_pmi(pmis_list, top_k):\n",
    "    pmis_dict = dict(pmis_list)\n",
    "    best_pmis = sorted(pmis_dict, key = lambda x:x[1], reverse = True)\n",
    "    print(best_pmis[:top_k])\n",
    "    return best_pmis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_collocations(tokens, top_best):\n",
    "    freq = Counter(tokens.split())\n",
    "    bigrams = list(nltk.bigrams(tokens.split()))\n",
    "    aprior = aprior_probability(freq)\n",
    "    bigrams_prob = bigram_probability(bigrams)\n",
    "    pmi = count_pmi(bigrams, bigrams_prob, aprior)\n",
    "    best = range_pmi(pmi, top_best)\n",
    "    return best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['мы рассказывали', 'совсем недавно', 'недавно мы', 'рассказывали о']\n"
     ]
    }
   ],
   "source": [
    "# top best left \n",
    "collocations_left = extract_collocations(tokens_left, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['истории об', 'архивной находке', 'его истории', 'об архивной', 'и его']\n"
     ]
    }
   ],
   "source": [
    "# top best right\n",
    "collocations_right = extract_collocations(tokens_right, 10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
