{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import pymorphy2 \n",
    "morph = pymorphy2.MorphAnalyzer()\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_texts_root = r'/home/nst/mount/data/share/yd/'\\\n",
    "                   'popular_science_texts_store/ner_markup/test_ner/test_texts/'\n",
    "    \n",
    "output_root = r'/home/nst/mount/data/share/yd/'\\\n",
    "               'popular_science_texts_store/ner_markup/test_ner/output_texts/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "file_name = 'nplus1.ru-news-2015-05-10-darkenergy.txt'\n",
    "sample_path_in = test_texts_root + file_name\n",
    "sample_path_out = output_root + file_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def slurp(path):\n",
    "    with open(path, 'r') as file_object:\n",
    "        return file_object.read()\n",
    "    \n",
    "def spit(path, text):\n",
    "    os.makedirs(os.path.dirname(path), exist_ok=True)\n",
    "    with open(path, 'w') as file_object:\n",
    "        return file_object.write(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text = slurp(sample_path_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5021"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spit(sample_path_out, text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def assertEqual(a, b):\n",
    "    if a != b:\n",
    "        raise AssertionError('expected \"%s\", actual \"%s\"' % (a, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def insert_tags(text, index_start, index_end):\n",
    "    initial_tag = '<>'\n",
    "    final_tag = '</>'\n",
    "    return text[:index_start] + initial_tag + text[index_start:index_end] \\\n",
    "        + final_tag + text[index_end:]\n",
    "\n",
    "assertEqual('ученый <>Хокинг</>', \n",
    "           insert_tags('ученый Хокинг', 7, 13))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Макфа']\n"
     ]
    }
   ],
   "source": [
    "text = 'Я ем макароны Макфа'\n",
    "pattern = re.compile(r'([А-ЯЁ][а-яё]+|[A-Z][a-z]+)')\n",
    "find_ner = re.finditer(pattern, text)\n",
    "n = [match.group() for match in find_ner]  \n",
    "print(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tag_text(text):\n",
    "    pattern = re.compile(r'([А-ЯЁ][а-яё]+|[A-Z][a-z]+)')\n",
    "    find_ner = re.finditer(pattern, text)\n",
    "    for item in reversed(list(find_ner)):\n",
    "        start_index, end_index = item.span()\n",
    "        word = item.group()\n",
    "        if names_processor.extract_keywords(word) == [word]:\n",
    "            text = insert_tags(text, start_index, end_index)\n",
    "    return text\n",
    "\n",
    "assertEqual('ученый <>Иванов</> из <>России</>', \n",
    "           tag_text('ученый Иванов из России'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_capitals(text):\n",
    "    \"\"\"\n",
    "    Makes a list of words in caplitals\n",
    "    \"\"\"\n",
    "    pattern = re.compile(r'([А-ЯЁ][а-яё]+|[A-Z][a-z]+)')\n",
    "    find_capitals = re.finditer(pattern, text)\n",
    "    capitals = [match.group() for match in find_capitals] \n",
    "    return capitals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_capitals_list(input_path, output_path):\n",
    "    \"\"\"\n",
    "    Creates a list of potential names\n",
    "    \"\"\"\n",
    "    capitals = []\n",
    "    texts = []\n",
    "    for root, dirs, files in os.walk(input_path):\n",
    "        for file_name in files:\n",
    "            file_path = input_path + file_name\n",
    "            text = slurp(file_path)\n",
    "            texts.append(text)\n",
    "    for text in texts:\n",
    "        words = extract_capitals(text)\n",
    "        capitals.extend(words)\n",
    "    capitals = set(capitals)\n",
    "    file_object = open(output_path, 'w')\n",
    "    for word in capitals:\n",
    "        file_object.write(\"%s\\n\" % word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compile_lemmas_list(input_path):\n",
    "    lemmas_list = []\n",
    "    for root, dirs, files in os.walk(input_path):\n",
    "        for file_name in files:\n",
    "            file_path = input_path + file_name\n",
    "            lemmas = slurp(file_path)\n",
    "            lemmas = re.findall(r'[а-яё]+', lemmas)\n",
    "            lemmas_list.extend(lemmas)\n",
    "    lemmas_list = list(set(lemmas_list))\n",
    "    return lemmas_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dicts_root = '/home/nst/mount/data/share/yd/popular_science_texts_store/ner_markup/slovnik/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmas = compile_lemmas_list(dicts_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "103258"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(lemmas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "texts_path = '/home/nst/mount/data/share/yd/popular_science_texts_store/'\\\n",
    "'ner_markup/test_ner/test_texts/'\n",
    "list_output = '/home/nst/mount/data/share/yd/popular_science_texts_store/ner_markup/capitals.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "make_capitals_list(texts_path, list_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(list_output, 'r') as fo:\n",
    "    capitals = fo.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10667"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(capitals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flashtext import KeywordProcessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if keyword_processor.extract_keywords(word) == [word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmas_processor = KeywordProcessor()\n",
    "lemmas_processor.add_keywords_from_list(lemmas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['домашний']"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmas_processor.extract_keywords('домашний')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_common_words(list_output_path):\n",
    "    potential_names = []\n",
    "    with open(list_output_path, 'r') as fo:\n",
    "        capitals = fo.readlines()\n",
    "    for word in tqdm(capitals):\n",
    "        word_lower = word.lower()\n",
    "        word_lower = word_lower.rstrip()\n",
    "        lemma = morph.parse(word_lower)[0].normal_form\n",
    "        if not lemmas_processor.extract_keywords(lemma) == [lemma]:\n",
    "            potential_names.append(word.rstrip())\n",
    "    potential_names = list(set(potential_names))\n",
    "    print('Deleted common words, current size:', len(potential_names))\n",
    "    return potential_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10667/10667 [00:03<00:00, 2692.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted common words, current size: 6257\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "proper_names = delete_common_words(list_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make draft files ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "names_processor = KeywordProcessor()\n",
    "names_processor.add_keywords_from_list(proper_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Гирц']"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "names_processor.extract_keywords('Гирц')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_draft_files(test_texts_root, output_root):\n",
    "    for root, dirs, files in os.walk(test_texts_root):\n",
    "        for file_name in files:\n",
    "            input_path = test_texts_root + file_name\n",
    "            output_path = output_root + file_name[:-3] + 'xml'\n",
    "            raw_text = slurp(input_path)\n",
    "            text = tag_text(raw_text)\n",
    "            spit(output_path, text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "make_draft_files(test_texts_root, output_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "draft_texts_root = r'/home/nst/mount/data/share/yd/'\\\n",
    "               'popular_science_texts_store/ner_markup/test_ner/output_texts/'\n",
    "    \n",
    "final_texts_root = r'/home/nst/mount/data/share/yd/popular_science_texts_store/'\\\n",
    "                'ner_markup/final_markup/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def change_tags(marked_texts_root):\n",
    "    start_tag = '<>'\n",
    "    final_tag = '</>'\n",
    "    for root, dirs, files in os.walk(marked_texts_root):\n",
    "        for file_name in files:\n",
    "            file_path = draft_texts_root + file_name\n",
    "            content = slurp(file_path)\n",
    "            marked_text = re.sub(start_tag, '&', content)\n",
    "            marked_text = re.sub(final_tag, '!&', marked_text)\n",
    "            output_path = final_texts_root + file_name[:-3] + 'txt'\n",
    "            spit(output_path, marked_text)\n",
    "    print('I\\'m done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make final changes ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm done\n"
     ]
    }
   ],
   "source": [
    "change_tags(draft_texts_root)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
