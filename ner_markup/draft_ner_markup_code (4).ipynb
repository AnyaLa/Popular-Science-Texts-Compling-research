{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import pymorphy2 \n",
    "morph = pymorphy2.MorphAnalyzer()\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_texts_root = r'/home/nst/mount/data/share/yd/'\\\n",
    "                   'popular_science_texts_store/ner_markup/test_ner/test_texts/'\n",
    "    \n",
    "output_root = r'/home/nst/mount/data/share/yd/'\\\n",
    "               'popular_science_texts_store/ner_markup/test_ner/output_texts/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def slurp(path):\n",
    "    with open(path, 'r') as file_object:\n",
    "        return file_object.read()\n",
    "    \n",
    "def spit(path, text):\n",
    "    os.makedirs(os.path.dirname(path), exist_ok=True)\n",
    "    with open(path, 'w') as file_object:\n",
    "        return file_object.write(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def assertEqual(a, b):\n",
    "    if a != b:\n",
    "        raise AssertionError('expected \"%s\", actual \"%s\"' % (a, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def insert_tags(text, index_start, index_end):\n",
    "    initial_tag = '<>'\n",
    "    final_tag = '</>'\n",
    "    return text[:index_start] + initial_tag + text[index_start:index_end] \\\n",
    "        + final_tag + text[index_end:]\n",
    "\n",
    "assertEqual('ученый <>Хокинг</>', \n",
    "           insert_tags('ученый Хокинг', 7, 13))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "expected \"ученый <>Иванов</> из <>России</>\", actual \"ученый Иванов из <>России</>\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-a9a8e3e1e5df>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m assertEqual('ученый <>Иванов</> из <>России</>', \n\u001b[0;32m---> 12\u001b[0;31m            tag_text('ученый Иванов из России'))\n\u001b[0m",
      "\u001b[0;32m<ipython-input-4-01d31886b786>\u001b[0m in \u001b[0;36massertEqual\u001b[0;34m(a, b)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0massertEqual\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0ma\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mAssertionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'expected \"%s\", actual \"%s\"'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m: expected \"ученый <>Иванов</> из <>России</>\", actual \"ученый Иванов из <>России</>\""
     ]
    }
   ],
   "source": [
    "def tag_text(text):\n",
    "    pattern = re.compile(r'([А-ЯЁ][а-яё]+|[A-Z][a-z]+)')\n",
    "    find_ner = re.finditer(pattern, text)\n",
    "    for item in reversed(list(find_ner)):\n",
    "        start_index, end_index = item.span()\n",
    "        word = item.group()\n",
    "        if names_processor.extract_keywords(word) == [word]:\n",
    "            text = insert_tags(text, start_index, end_index)\n",
    "    return text\n",
    "\n",
    "assertEqual('ученый <>Иванов</> из <>России</>', \n",
    "           tag_text('ученый Иванов из России'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_capitals(text):\n",
    "    \"\"\"\n",
    "    Makes a list of words in caplitals\n",
    "    \"\"\"\n",
    "    pattern = re.compile(r'([А-ЯЁ][а-яё]+|[A-Z][a-z]+)')\n",
    "    find_capitals = re.finditer(pattern, text)\n",
    "    capitals = [match.group() for match in find_capitals] \n",
    "    return capitals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_capitals_list(input_path, output_path):\n",
    "    \"\"\"\n",
    "    Creates a list of potential names\n",
    "    \"\"\"\n",
    "    capitals = []\n",
    "    texts = []\n",
    "    for root, dirs, files in os.walk(input_path):\n",
    "        for file_name in files:\n",
    "            file_path = input_path + file_name\n",
    "            text = slurp(file_path)\n",
    "            texts.append(text)\n",
    "    for text in texts:\n",
    "        words = extract_capitals(text)\n",
    "        capitals.extend(words)\n",
    "    capitals = set(capitals)\n",
    "    file_object = open(output_path, 'w')\n",
    "    for word in capitals:\n",
    "        file_object.write(\"%s\\n\" % word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compile_lemmas_list(input_path):\n",
    "    lemmas_list = []\n",
    "    for root, dirs, files in os.walk(input_path):\n",
    "        for file_name in files:\n",
    "            file_path = input_path + file_name\n",
    "            lemmas = slurp(file_path)\n",
    "            lemmas = re.findall(r'[а-яё]+', lemmas)\n",
    "            lemmas_list.extend(lemmas)\n",
    "    lemmas_list = list(set(lemmas_list))\n",
    "    return lemmas_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dicts_root = '/home/nst/mount/data/share/yd/popular_science_texts_store/ner_markup/slovnik/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lemmas = compile_lemmas_list(dicts_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "103258"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(lemmas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "texts_path = '/home/nst/mount/data/share/yd/popular_science_texts_store/'\\\n",
    "'ner_markup/test_ner/test_texts/'\n",
    "list_output = '/home/nst/mount/data/share/yd/popular_science_texts_store/ner_markup/capitals.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "make_capitals_list(texts_path, list_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(list_output, 'r') as fo:\n",
    "    capitals = fo.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1465"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(capitals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from flashtext import KeywordProcessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lemmas_processor = KeywordProcessor()\n",
    "lemmas_processor.add_keywords_from_list(lemmas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['домашний']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmas_processor.extract_keywords('домашний')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def delete_common_words(list_output_path):\n",
    "    potential_names = []\n",
    "    with open(list_output_path, 'r') as fo:\n",
    "        capitals = fo.readlines()\n",
    "    for word in tqdm(capitals):\n",
    "        word_lower = word.lower()\n",
    "        word_lower = word_lower.rstrip()\n",
    "        lemma = morph.parse(word_lower)[0].normal_form\n",
    "        if not lemmas_processor.extract_keywords(lemma) == [lemma]:\n",
    "            potential_names.append(word.rstrip())\n",
    "    potential_names = list(set(potential_names))\n",
    "    print('Deleted common words, current size:', len(potential_names))\n",
    "    return potential_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1465/1465 [00:00<00:00, 1979.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted common words, current size: 730\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "proper_names = delete_common_words(list_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "names_processor = KeywordProcessor()\n",
    "names_processor.add_keywords_from_list(proper_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "names_processor.extract_keywords('Энштейн')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make draft files ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_draft_files(test_texts_root, output_root):\n",
    "    for root, dirs, files in os.walk(test_texts_root):\n",
    "        for file_name in files:\n",
    "            input_path = test_texts_root + file_name\n",
    "            output_path = output_root + file_name[:-3] + 'xml'\n",
    "            raw_text = slurp(input_path)\n",
    "            text = tag_text(raw_text)\n",
    "            spit(output_path, text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_draft_files(test_texts_root, output_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "draft_texts_root = r'/home/nst/mount/data/share/yd/'\\\n",
    "               'popular_science_texts_store/ner_markup/test_ner/output_texts/'\n",
    "    \n",
    "final_texts_root = r'/home/nst/mount/data/share/yd/popular_science_texts_store/'\\\n",
    "                'ner_markup/final_markup/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def change_tags(marked_texts_root):\n",
    "    start_tag = '<>'\n",
    "    final_tag = '</>'\n",
    "    for root, dirs, files in os.walk(marked_texts_root):\n",
    "        for file_name in files:\n",
    "            file_path = draft_texts_root + file_name\n",
    "            content = slurp(file_path)\n",
    "            marked_text = re.sub(start_tag, '&', content)\n",
    "            marked_text = re.sub(final_tag, '!&', marked_text)\n",
    "            output_path = final_texts_root + file_name[:-3] + 'txt'\n",
    "            spit(output_path, marked_text)\n",
    "    print('I\\'m done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make final changes ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm done\n"
     ]
    }
   ],
   "source": [
    "change_tags(draft_texts_root)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
